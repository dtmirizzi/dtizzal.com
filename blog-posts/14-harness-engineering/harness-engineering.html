<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <title>DT Mirizzi - Harness Engineering: The Discipline That Replaces Writing Code</title>

  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:ital,wght@0,400;0,700;1,400;1,700&display=swap"
    rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined" rel="stylesheet" />

  <style>
    body {
      font-family: 'Courier Prime', monospace;
      line-height: 1.6;
      background-color: #1a1a1a;
      color: #e0e0e0;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }

    h1,
    h2,
    h3 {
      color: #ffffff;
      font-weight: 700;
    }

    a {
      color: #bbbbbb;
      text-decoration: none;
      border-bottom: 1px dotted #777;
    }

    a:hover {
      color: #ffffff;
      border-bottom: 1px solid #fff;
    }

    .my-image {
      width: 100%;
      max-width: 800px;
      height: auto;
      display: block;
      margin: 20px auto;
      filter: brightness(0.9);
      border-radius: 10px;
    }

    @media (min-width: 600px) {
      .my-image {
        width: 100%;
        max-width: 800px;
      }
    }

    .footnotes {
      margin-top: 40px;
      border-top: 1px solid #333;
      padding-top: 20px;
      font-size: 0.95rem;
      color: #cfcfcf;
    }

    .footnotes ol {
      padding-left: 20px;
    }

    .footnotes li {
      margin: 8px 0;
    }

    /* SVG theme-aware styles */
    .diagram-text { font-family: 'Courier Prime', monospace; fill: #e0e0e0; }
    .diagram-title { font-size: 13px; font-weight: 700; fill: #ffffff; }
    .diagram-label { font-size: 13px; }
    .diagram-small { font-size: 11px; fill: #999999; }
    .diagram-accent { fill: #2dc7ff; }
    .diagram-accent-stroke { stroke: #2dc7ff; }
    .diagram-dim { fill: #888888; }
    .diagram-dim-stroke { stroke: #555555; }
    .diagram-warn { fill: #ff6b6b; }
    .diagram-warn-stroke { stroke: #ff6b6b; }
    .diagram-ok { fill: #51cf66; }
    .diagram-ok-stroke { stroke: #51cf66; }
    .diagram-box { fill: none; stroke: #555; stroke-width: 1.5; }
    .diagram-box-accent { fill: none; stroke: #2dc7ff; stroke-width: 1.5; }
    .diagram-box-warn { fill: none; stroke: #ff6b6b; stroke-width: 1.5; }
    .diagram-box-ok { fill: none; stroke: #51cf66; stroke-width: 1.5; }
    .diagram-bg-accent { fill: rgba(45,199,255,0.08); }
    .diagram-bg-warn { fill: rgba(255,107,107,0.08); }
    .diagram-bg-ok { fill: rgba(81,207,102,0.08); }
    .diagram-divider { stroke: #333; stroke-width: 1; stroke-dasharray: 6,4; }

    /* Light mode SVG overrides */
    body.light-mode .diagram-text { fill: #1a1a1a; }
    body.light-mode .diagram-title { fill: #1a1a1a; }
    body.light-mode .diagram-small { fill: #666666; }
    body.light-mode .diagram-accent { fill: #0E8AC8; }
    body.light-mode .diagram-accent-stroke { stroke: #0E8AC8; }
    body.light-mode .diagram-dim { fill: #888888; }
    body.light-mode .diagram-dim-stroke { stroke: #aaaaaa; }
    body.light-mode .diagram-warn { fill: #d63031; }
    body.light-mode .diagram-warn-stroke { stroke: #d63031; }
    body.light-mode .diagram-ok { fill: #2d8a3e; }
    body.light-mode .diagram-ok-stroke { stroke: #2d8a3e; }
    body.light-mode .diagram-box { stroke: #aaaaaa; }
    body.light-mode .diagram-box-accent { stroke: #0E8AC8; }
    body.light-mode .diagram-box-warn { stroke: #d63031; }
    body.light-mode .diagram-box-ok { stroke: #2d8a3e; }
    body.light-mode .diagram-bg-accent { fill: rgba(14,138,200,0.08); }
    body.light-mode .diagram-bg-warn { fill: rgba(214,48,49,0.08); }
    body.light-mode .diagram-bg-ok { fill: rgba(45,138,62,0.08); }
    body.light-mode .diagram-divider { stroke: #cccccc; }

    blockquote {
      border-left: 3px solid #2dc7ff;
      margin: 1.5em 0;
      padding: 0.5em 1em;
      font-style: italic;
      color: #bbb;
    }

    body.light-mode blockquote {
      border-left-color: #0E8AC8;
      color: #555;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-size: 0.9rem;
    }

    th, td {
      padding: 10px 14px;
      text-align: left;
      border-bottom: 1px solid #333;
    }

    th {
      color: #ffffff;
      font-weight: 700;
      border-bottom: 2px solid #555;
    }

    body.light-mode th {
      color: #1a1a1a;
      border-bottom-color: #aaa;
    }

    body.light-mode td {
      border-bottom-color: #ccc;
    }

    code {
      background: rgba(255,255,255,0.08);
      padding: 2px 6px;
      border-radius: 3px;
      font-size: 0.9em;
    }

    body.light-mode code {
      background: rgba(0,0,0,0.06);
    }
  </style>

  <meta name="title" content="DT Mirizzi" />
  <meta name="description" content="Harness Engineering: The Discipline That Replaces Writing Code. How context engineering, architectural constraints, and feedback loops are becoming the real job." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://dtizzal.com/" />
  <meta property="og:title" content="Harness Engineering: The Discipline That Replaces Writing Code" />
  <meta property="og:description"
    content="The engineer's job is no longer to write code. It's to design environments, specify intent, and build feedback loops. This is harness engineering." />
  <meta property="og:image" content="https://dtizzal.com/img/fb.png" />
  <meta property="twitter:card" content="summary_large_image" />
  <meta property="twitter:url" content="https://dtizzal.com/" />
  <meta property="twitter:title" content="Harness Engineering: The Discipline That Replaces Writing Code" />
  <meta property="twitter:description"
    content="The engineer's job is no longer to write code. It's to design environments, specify intent, and build feedback loops. This is harness engineering." />
  <meta property="twitter:image" content="https://dtizzal.com/img/fb.png" />

  <link href="/css/style.css" rel="stylesheet" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z4WRP0KWGX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-Z4WRP0KWGX');
  </script>
</head>

<body>
  <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle dark mode">
    <span class="material-symbols-outlined" id="theme-icon">light_mode</span>
  </button>

  <a href="/blog.html">&#x27F5; back</a>

  <video loop muted autoplay playsinline webkit-playsinline preload="auto">
    <source src="/img/Background.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>

  <div class="job-helper">
    <div id="article-content">
      <h1><strong>Harness Engineering: The Discipline That Replaces Writing Code</strong></h1>

      <p>
        A team of three engineers built a million-line codebase in five months. Zero hand-written code. They
        averaged 3.5 pull requests per engineer per day. Separately, a solo developer shipped 6,600+
        commits per month running 5-10 agents simultaneously. Elsewhere, an internal system at a major
        payments company produces over 1,000 merged pull requests per week via Slack-based task
        automation.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <p>
        These aren't cherry-picked demos. They're converging signals from independent teams arriving at the
        same conclusion: the engineer's primary job is no longer writing code. It's designing the environment
        in which agents write code. The term for this is <strong>harness engineering</strong>, and it's the
        most important discipline in software that most people haven't heard of yet.
      </p>

      <h2><strong>What is a harness?</strong></h2>

      <p>
        The term was popularized by Mitchell Hashimoto and formalized in Birgitta Bockeler's analysis on Martin
        Fowler's site. The definition: "the tooling and practices we can use to keep AI agents in check,"
        mixing deterministic and LLM-based approaches. The operational principle, per Hashimoto: "anytime you
        find an agent makes a mistake, you take the time to engineer a solution such that the agent never makes
        that mistake again."<sup><a href="#fn2">[2]</a></sup><sup><a href="#fn1">[1]</a></sup>
      </p>

      <p>
        A harness is not a prompt. It's not a system message. It's not a YAML configuration file. It's the
        entire engineered environment surrounding an agent: the context it receives, the constraints it operates
        within, the tools it can access, the feedback loops that correct it, and the verification systems that
        validate its output. If the model is the engine, the harness is the car: steering, brakes, transmission,
        instruments, and road markings included.
      </p>

      <!-- Diagram: The Harness Components -->
      <svg viewBox="0 0 760 440" xmlns="http://www.w3.org/2000/svg" style="width:100%;max-width:760px;display:block;margin:30px auto;">
        <defs>
          <marker id="compArrow" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
            <path d="M0,0 L8,3 L0,6" class="diagram-accent-stroke" fill="none" stroke-width="1.2"/>
          </marker>
          <marker id="feedbackArrow" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
            <path d="M0,0 L8,3 L0,6" class="diagram-dim-stroke" fill="none" stroke-width="1.2"/>
          </marker>
        </defs>

        <text x="380" y="28" text-anchor="middle" class="diagram-text diagram-title diagram-accent">The Harness</text>

        <!-- Center: Agent -->
        <rect x="290" y="180" width="180" height="60" rx="8" class="diagram-box"/>
        <text x="380" y="207" text-anchor="middle" class="diagram-text diagram-label">LLM Agent</text>
        <text x="380" y="227" text-anchor="middle" class="diagram-text diagram-small">capable but untrusted</text>

        <!-- Top: Context Engineering -->
        <rect x="245" y="50" width="270" height="60" rx="6" class="diagram-box-accent diagram-bg-accent"/>
        <text x="380" y="75" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Context Engineering</text>
        <text x="380" y="95" text-anchor="middle" class="diagram-text diagram-small">what the model sees: docs, code, tools, memory</text>
        <line x1="380" y1="110" x2="380" y2="180" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#compArrow)"/>

        <!-- Left: Architectural Constraints -->
        <rect x="20" y="165" width="230" height="60" rx="6" class="diagram-box-accent diagram-bg-accent"/>
        <text x="135" y="190" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Architectural Constraints</text>
        <text x="135" y="210" text-anchor="middle" class="diagram-text diagram-small">linters, structural tests, boundaries</text>
        <line x1="250" y1="200" x2="290" y2="205" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#compArrow)"/>

        <!-- Right: Tools -->
        <rect x="510" y="165" width="230" height="60" rx="6" class="diagram-box-accent diagram-bg-accent"/>
        <text x="625" y="190" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Tool Access</text>
        <text x="625" y="210" text-anchor="middle" class="diagram-text diagram-small">MCP servers, APIs, test runners</text>
        <line x1="510" y1="200" x2="470" y2="205" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#compArrow)"/>

        <!-- Bottom-left: Verification -->
        <rect x="80" y="310" width="230" height="60" rx="6" class="diagram-box-accent diagram-bg-accent"/>
        <text x="195" y="335" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Verification</text>
        <text x="195" y="355" text-anchor="middle" class="diagram-text diagram-small">tests, browser automation, e2e checks</text>
        <line x1="270" y1="310" x2="340" y2="240" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#compArrow)"/>

        <!-- Bottom-right: Garbage Collection -->
        <rect x="450" y="310" width="230" height="60" rx="6" class="diagram-box-accent diagram-bg-accent"/>
        <text x="565" y="335" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Garbage Collection</text>
        <text x="565" y="355" text-anchor="middle" class="diagram-text diagram-small">entropy detection, cleanup agents</text>
        <line x1="490" y1="310" x2="420" y2="240" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#compArrow)"/>

        <!-- Feedback loop: curving from bottom back to top -->
        <path d="M380,240 L380,280 L710,280 L710,80 L515,80" fill="none" class="diagram-dim-stroke" stroke-width="1.2" stroke-dasharray="5,3" marker-end="url(#feedbackArrow)"/>
        <text x="720" y="185" text-anchor="middle" class="diagram-text diagram-small diagram-dim" transform="rotate(90, 720, 185)">feedback loop</text>

        <!-- Bottom label -->
        <text x="380" y="420" text-anchor="middle" class="diagram-text diagram-small">Every agent failure becomes a harness improvement.</text>
        <text x="380" y="436" text-anchor="middle" class="diagram-text diagram-small diagram-accent">The harness gets smarter. The model stays the same.</text>
      </svg>

      <p>
        The harness has three core components, each doing different work:
      </p>

      <h2><strong>Component 1: context engineering</strong></h2>

      <p>
        Context engineering is the practice of curating the optimal set of information available to the model
        during inference. Not just the prompt, but everything that lands in the context window: background
        knowledge, retrieved data, tool descriptions, structured inputs, memory from previous sessions, and
        the code itself. The goal is finding the smallest set of high-signal tokens that maximizes desired
        outcomes.<sup><a href="#fn3">[3]</a></sup>
      </p>

      <p>
        This matters more than most people realize. LLMs face an architectural constraint from the transformer
        attention mechanism where every token attends to every other token, creating n-squared relationships.
        As context grows, models experience degraded recall and reasoning precision -- a phenomenon called
        "context rot." Larger context windows don't solve this; they just move the degradation curve. Context
        is a finite resource even when the window is a million tokens.<sup><a href="#fn3">[3]</a></sup>
      </p>

      <p>
        The most sophisticated implementation I've seen is documented in a recent paper on codified context
        infrastructure. The team built a three-tier architecture for a 108,000-line C# distributed
        system:<sup><a href="#fn4">[4]</a></sup>
      </p>

      <!-- Diagram: Three-Tier Context Architecture -->
      <svg viewBox="0 0 760 380" xmlns="http://www.w3.org/2000/svg" style="width:100%;max-width:760px;display:block;margin:30px auto;">
        <defs>
          <marker id="tierArrow" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
            <path d="M0,0 L8,3 L0,6" class="diagram-accent-stroke" fill="none" stroke-width="1.2"/>
          </marker>
        </defs>

        <text x="380" y="28" text-anchor="middle" class="diagram-text diagram-title">Three-Tier Codified Context</text>

        <!-- Tier 1: Hot Memory -->
        <rect x="130" y="50" width="500" height="70" rx="6" class="diagram-box-accent diagram-bg-accent"/>
        <text x="380" y="72" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Tier 1: Hot Memory (Constitution)</text>
        <text x="380" y="90" text-anchor="middle" class="diagram-text diagram-small">~660 lines, always loaded every session</text>
        <text x="380" y="106" text-anchor="middle" class="diagram-text diagram-small">conventions, build commands, arch summaries, trigger tables</text>

        <line x1="380" y1="120" x2="380" y2="145" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#tierArrow)"/>

        <!-- Tier 2: Domain Specialists -->
        <rect x="130" y="145" width="500" height="70" rx="6" class="diagram-box-accent"/>
        <text x="380" y="167" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Tier 2: Domain Specialist Agents</text>
        <text x="380" y="185" text-anchor="middle" class="diagram-text diagram-small">19 agents, ~9,300 lines total, invoked per task</text>
        <text x="380" y="201" text-anchor="middle" class="diagram-text diagram-small">50%+ of each spec is domain knowledge, not behavioral instructions</text>

        <line x1="380" y1="215" x2="380" y2="240" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#tierArrow)"/>

        <!-- Tier 3: Cold Memory -->
        <rect x="130" y="240" width="500" height="70" rx="6" class="diagram-box"/>
        <text x="380" y="262" text-anchor="middle" class="diagram-text diagram-label">Tier 3: Cold Memory (Knowledge Base)</text>
        <text x="380" y="280" text-anchor="middle" class="diagram-text diagram-small">34 spec documents, ~16,250 lines, queried on-demand via MCP</text>
        <text x="380" y="296" text-anchor="middle" class="diagram-text diagram-small">symptom-cause-fix tables, conversion formulas, failure modes</text>

        <!-- Stats -->
        <text x="380" y="340" text-anchor="middle" class="diagram-text diagram-small">Total context infrastructure: 54 files, ~26,200 lines</text>
        <text x="380" y="358" text-anchor="middle" class="diagram-text diagram-small diagram-accent">24.2% of the entire codebase is context for agents, not runtime code</text>
      </svg>

      <p>
        A quarter of the codebase exists solely to make agents effective. The hot memory tier -- a single ~660-line
        file loaded at the start of every session -- contains conventions, known failure modes, and trigger tables
        that route tasks to the right specialist agent based on file patterns. The cold memory tier holds 34
        specification documents queryable on demand, including symptom-cause-fix tables that encode debugging
        knowledge the way a senior engineer carries it in their head.<sup><a href="#fn4">[4]</a></sup>
      </p>

      <p>
        The results are striking. Their save-system specification, the most-referenced document at 74 sessions,
        prevented save-related bugs across four weeks without a single failure. When they found a subsystem
        lacking documentation, it was precisely the subsystem where agents introduced the most
        regressions.<sup><a href="#fn4">[4]</a></sup>
      </p>

      <p>
        For long-running agents that span multiple context windows, the techniques get more interesting.
        Compaction summarizes conversation contents when approaching context limits, preserving architectural
        decisions while discarding redundant tool outputs. Structured note-taking uses persistent external files
        (progress trackers, to-do lists) consulted across context resets. Multi-agent architectures assign
        specialized sub-agents to focused tasks with clean context windows, returning condensed summaries of
        1,000-2,000 tokens to the coordinator.<sup><a href="#fn3">[3]</a></sup>
      </p>

      <p>
        One team found that initializer agents -- agents that run first to generate comprehensive feature
        requirement files before any coding begins -- transformed their workflow. The initializer creates a JSON
        file cataloging 200+ discrete features, each marked as "failing." The coding agent receives strict
        instructions prohibiting deletion of features from this list, preventing it from hiding functionality
        gaps by simply removing the requirement.<sup><a href="#fn5">[5]</a></sup>
      </p>

      <p>
        Context engineering is where the term "prompt engineering" needed to go but couldn't because prompting
        implies a single interaction. Context engineering implies a system. And systems are what engineers build.
      </p>

      <h2><strong>Component 2: architectural constraints</strong></h2>

      <p>
        Here's the counterintuitive finding: constraining agents makes them more productive, not less. The teams
        with the highest throughput are the ones enforcing the strictest architectural boundaries.
      </p>

      <p>
        One team enforced a rigid layered architecture where each business domain flows through a fixed set of
        layers: Types, Config, Repo, Service, Runtime, UI. Dependencies only flow in one direction. This is
        monitored by both custom deterministic linters and LLM-based review agents. When an agent tries to
        import a UI module from the Repo layer, a linter catches it before the code is ever
        committed.<sup><a href="#fn2">[2]</a></sup>
      </p>

      <p>
        The linter error messages double as remediation instructions. The violation isn't just flagged; the
        error tells the agent exactly how to fix it. This creates a self-correcting loop: the agent makes
        an architectural mistake, the linter explains why it's wrong and how to fix it, and the agent corrects
        itself without human intervention. The linter is teaching the agent while it
        works.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <!-- Diagram: The Self-Correcting Loop -->
      <svg viewBox="0 0 760 280" xmlns="http://www.w3.org/2000/svg" style="width:100%;max-width:760px;display:block;margin:30px auto;">
        <defs>
          <marker id="loopArrow" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
            <path d="M0,0 L8,3 L0,6" class="diagram-accent-stroke" fill="none" stroke-width="1.2"/>
          </marker>
          <marker id="loopWarn" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
            <path d="M0,0 L8,3 L0,6" class="diagram-warn-stroke" fill="none" stroke-width="1.2"/>
          </marker>
        </defs>

        <text x="380" y="28" text-anchor="middle" class="diagram-text diagram-title">The Self-Correcting Loop</text>

        <!-- Step 1: Agent writes code -->
        <rect x="40" y="65" width="180" height="50" rx="5" class="diagram-box"/>
        <text x="130" y="87" text-anchor="middle" class="diagram-text diagram-label">Agent writes code</text>
        <text x="130" y="105" text-anchor="middle" class="diagram-text diagram-small">imports UI from Repo</text>

        <!-- Arrow to linter -->
        <line x1="220" y1="90" x2="290" y2="90" class="diagram-warn-stroke" stroke-width="1.5" marker-end="url(#loopWarn)"/>

        <!-- Step 2: Linter catches -->
        <rect x="290" y="65" width="180" height="50" rx="5" class="diagram-box-warn"/>
        <text x="380" y="87" text-anchor="middle" class="diagram-text diagram-label diagram-warn">Linter rejects</text>
        <text x="380" y="105" text-anchor="middle" class="diagram-text diagram-small">"use Service layer instead"</text>

        <!-- Arrow to fix -->
        <line x1="470" y1="90" x2="540" y2="90" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#loopArrow)"/>

        <!-- Step 3: Agent fixes -->
        <rect x="540" y="65" width="180" height="50" rx="5" class="diagram-box-accent"/>
        <text x="630" y="87" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Agent self-corrects</text>
        <text x="630" y="105" text-anchor="middle" class="diagram-text diagram-small">no human intervention</text>

        <!-- Second row: what accumulates -->
        <rect x="130" y="160" width="500" height="50" rx="5" class="diagram-box-accent diagram-bg-accent"/>
        <text x="380" y="182" text-anchor="middle" class="diagram-text diagram-label diagram-accent">Harness Improvement</text>
        <text x="380" y="200" text-anchor="middle" class="diagram-text diagram-small">new linter rule added, agent never makes this class of mistake again</text>

        <!-- Arrow from loop to improvement -->
        <line x1="380" y1="115" x2="380" y2="160" class="diagram-accent-stroke" stroke-width="1.5" marker-end="url(#loopArrow)"/>

        <!-- Bottom -->
        <text x="380" y="245" text-anchor="middle" class="diagram-text diagram-small">The model doesn't learn. The harness does.</text>
        <text x="380" y="265" text-anchor="middle" class="diagram-text diagram-small diagram-accent">Institutional knowledge lives in the constraints, not the weights.</text>
      </svg>

      <p>
        This also extends to what Bockeler calls "taste invariants" -- a small set of non-negotiable conventions
        that aren't about correctness but about coherence: structured logging, naming conventions for schemas and
        types, platform-specific reliability requirements. These are the kind of things a senior engineer
        enforces in code review through sheer cultural pressure. In a harness, they're encoded as automated
        checks.<sup><a href="#fn2">[2]</a></sup>
      </p>

      <p>
        The paradox is real: increasing trust and reliability in AI-generated code requires constraining the
        solution space rather than expanding it. Agents are most effective in environments with strict boundaries
        and predictable structure. This is the opposite of the "give the AI maximum flexibility" intuition that
        most people start with.
      </p>

      <h2><strong>Component 3: garbage collection</strong></h2>

      <p>
        Code generated by AI accumulates entropy differently than human-written code, but it still accumulates
        entropy. Between 2020 and 2024, there was an 8-fold increase in code blocks containing five or more
        duplicated lines. 2024 was the first year where the number of copy-pasted lines exceeded the number
        of refactored lines.<sup><a href="#fn6">[6]</a></sup> AI models prioritize local functional correctness
        over global architectural coherence, generating code that works in isolation but degrades the codebase
        holistically.<sup><a href="#fn7">[7]</a></sup>
      </p>

      <p>
        Garbage collection in harness engineering means running periodic agents whose sole purpose is to find
        and fix decay: documentation inconsistencies, architectural constraint violations, dead code, duplicated
        patterns, naming drift. These aren't the coding agents. They're the cleanup crew. They run on schedules
        or triggers, not on developer prompts.<sup><a href="#fn2">[2]</a></sup>
      </p>

      <p>
        This is the component most teams skip and then regret. Without active entropy management, the codebase
        degrades until the agents themselves start struggling -- their context fills with inconsistent patterns,
        conflicting conventions, and dead ends. The garbage collection agents close the loop: they keep the
        codebase in a state where the coding agents can remain effective. The harness maintains itself.
      </p>

      <h2><strong>The new job description</strong></h2>

      <p>
        If the harness is doing this much work, what are the humans doing?
      </p>

      <p>
        The job splits into two halves that operate simultaneously, not sequentially. The first half is
        <strong>building the environment</strong>: creating structure, tools, and feedback mechanisms so agents
        proceed reliably. The second half is <strong>managing the work</strong>: planning, directing, and
        reviewing agent output at the architectural level.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <!-- Diagram: The Engineer's New Job -->
      <svg viewBox="0 0 760 340" xmlns="http://www.w3.org/2000/svg" style="width:100%;max-width:760px;display:block;margin:30px auto;">
        <text x="380" y="28" text-anchor="middle" class="diagram-text diagram-title">The Engineer's Job, Before and After</text>

        <!-- Divider -->
        <line x1="380" y1="45" x2="380" y2="310" class="diagram-divider"/>

        <!-- Left: Before -->
        <text x="190" y="60" text-anchor="middle" class="diagram-text diagram-label diagram-dim">Before</text>

        <rect x="50" y="80" width="280" height="34" rx="4" class="diagram-box"/>
        <text x="190" y="102" text-anchor="middle" class="diagram-text diagram-small">Read requirements</text>

        <rect x="50" y="122" width="280" height="34" rx="4" class="diagram-box"/>
        <text x="190" y="144" text-anchor="middle" class="diagram-text diagram-small">Design solution</text>

        <rect x="50" y="164" width="280" height="34" rx="4" class="diagram-box"/>
        <text x="190" y="186" text-anchor="middle" class="diagram-text diagram-small">Write code</text>

        <rect x="50" y="206" width="280" height="34" rx="4" class="diagram-box"/>
        <text x="190" y="228" text-anchor="middle" class="diagram-text diagram-small">Debug code</text>

        <rect x="50" y="248" width="280" height="34" rx="4" class="diagram-box"/>
        <text x="190" y="270" text-anchor="middle" class="diagram-text diagram-small">Review others' code</text>

        <text x="190" y="310" text-anchor="middle" class="diagram-text diagram-small diagram-dim">most time: writing and debugging</text>

        <!-- Right: After -->
        <text x="570" y="60" text-anchor="middle" class="diagram-text diagram-label diagram-accent">After</text>

        <rect x="430" y="80" width="280" height="34" rx="4" class="diagram-box-accent"/>
        <text x="570" y="102" text-anchor="middle" class="diagram-text diagram-small">Design architecture + constraints</text>

        <rect x="430" y="122" width="280" height="34" rx="4" class="diagram-box-accent"/>
        <text x="570" y="144" text-anchor="middle" class="diagram-text diagram-small">Write context docs + AGENTS.md</text>

        <rect x="430" y="164" width="280" height="34" rx="4" class="diagram-box-accent"/>
        <text x="570" y="186" text-anchor="middle" class="diagram-text diagram-small">Build linters + structural tests</text>

        <rect x="430" y="206" width="280" height="34" rx="4" class="diagram-box-accent"/>
        <text x="570" y="228" text-anchor="middle" class="diagram-text diagram-small">Plan tasks + orchestrate agents</text>

        <rect x="430" y="248" width="280" height="34" rx="4" class="diagram-box-accent"/>
        <text x="570" y="270" text-anchor="middle" class="diagram-text diagram-small">Review output at arch level</text>

        <text x="570" y="310" text-anchor="middle" class="diagram-text diagram-small diagram-accent">most time: environment design + review</text>
      </svg>

      <p>
        The critical practice everyone converges on: <strong>separate planning from execution.</strong> One
        practitioner calls it "the single most important thing I do." The pattern is consistent across teams:
        spend significant time on planning and specification <em>before</em> any agent writes a line of code.
        Define the feature list. Specify the architectural boundaries. Write the context documents. Then let
        the agents execute within those constraints.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <p>
        The review bar matters too. The teams shipping the most code maintain the same review standards for
        agent output as for human output. One practitioner acts as architectural gatekeeper while trusting
        agents with implementation details, treating them as "experienced subcontractors" rather than junior
        developers. The trust is scoped: high trust for implementation within constraints, zero trust for
        architectural decisions.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <p>
        There's an emerging convention around this: <code>AGENTS.md</code>. A Markdown file at the repository
        root that coding agents automatically read at the start of every session. It tells agents about build
        steps, testing commands, coding conventions, architectural constraints, and common pitfalls. The
        critical pattern: update it every time agents fail. The file is a living document of institutional
        knowledge encoded for machine consumption.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <h2><strong>Orchestration models</strong></h2>

      <p>
        How humans manage agents varies, and two models are emerging:
      </p>

      <p>
        <strong>Attended parallelization</strong> means actively managing 3-4 concurrent agent sessions. The
        developer is in the loop during execution, steering, course-correcting, and spawning new agents as
        tasks complete. This is the solo developer model: high throughput, high cognitive load, but maximum
        control over architectural coherence.
      </p>

      <p>
        <strong>Unattended parallelization</strong> means posting tasks and re-entering only at the review
        stage. Developers describe tasks via Slack or a ticket interface, agents execute asynchronously, and
        the developer reviews completed PRs. This is the team model: lower per-developer throughput but
        massive parallelism across the organization. One company's system processes 1,000+ merged PRs per
        week this way.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <p>
        Both models require the same harness infrastructure. The difference is how tightly coupled the
        human is to the execution loop. Both models fail identically without strong constraints, context,
        and verification: the agents drift, the code decays, and the review burden overwhelms the humans.
      </p>

      <h2><strong>The entropy problem</strong></h2>

      <p>
        Here's the part nobody wants to talk about: AI-generated code might be making developers worse.
      </p>

      <p>
        A randomized controlled trial with 52 junior engineers found that AI-assisted developers scored
        approximately 17% lower on comprehension tests compared to manual coders. The AI group averaged 50%
        on quiz scores versus 67% for the manual group. The largest gap was in debugging questions.
        Developers who delegated code generation to AI scored below 40%, while those who used AI for
        conceptual questions achieved 65% or higher.<sup><a href="#fn8">[8]</a></sup>
      </p>

      <p>
        This matters for harness engineering because the harness assumes humans who can do architectural
        review, design constraints, and diagnose agent failures. If the next generation of engineers develops
        weaker debugging instincts because they delegated implementation to agents during their formative
        years, the review layer of the harness weakens. The constraint quality degrades. The garbage
        collection agents miss things because the humans overseeing them don't recognize the patterns.
      </p>

      <p>
        The codebase-level entropy is measurable too. Research shows AI models produce approximately 90-93%
        code smells, 5-8% bugs, and around 2% security vulnerabilities across generated code. The incentive
        structure encourages accepting quick, duplicated snippets over refactored, coherent
        patterns.<sup><a href="#fn6">[6]</a></sup><sup><a href="#fn7">[7]</a></sup> Without a strong harness
        -- without linters that catch duplication, structural tests that enforce modularity, and garbage
        collection agents that clean up drift -- the codebase decays faster than humans can review it.
      </p>

      <p>
        This is why harness engineering isn't optional. It's not a nice-to-have for teams that want to be
        extra careful. It's the structural requirement for AI-assisted development that doesn't collapse under
        its own entropy. The harness is load-bearing.
      </p>

      <h2><strong>What's still unsolved</strong></h2>

      <p>
        Harness engineering is a discipline that's maybe 18 months old as a named practice. The convergence
        is real but the gaps are significant.
      </p>

      <p>
        <strong>Brownfield codebases.</strong> Everything documented so far works best on greenfield projects.
        Retrofitting a harness to a legacy system with inconsistent structure, undocumented conventions, and
        ten years of accumulated entropy is a different problem entirely. It's analogous to turning on a linter
        for the first time on a million-line codebase: the signal-to-noise ratio is immediately
        overwhelming.<sup><a href="#fn1">[1]</a></sup><sup><a href="#fn2">[2]</a></sup>
      </p>

      <p>
        <strong>Verification at scale.</strong> Agents routinely mark features "complete" without proper
        end-to-end testing. Vision and tool access limitations create verification gaps. One team found that
        agents didn't reliably catch bugs until explicitly instructed to use browser automation via Puppeteer,
        shifting from unit tests and curl to actual end-to-end user workflows.<sup><a href="#fn5">[5]</a></sup>
      </p>

      <p>
        <strong>Context infrastructure maintenance.</strong> The three-tier codified context system requires
        ~1-2 hours per week of maintenance, primarily updating specifications during code changes.
        Specification staleness was identified as the primary failure mode: when the docs drift from the code,
        the agents start making mistakes that the harness was designed to
        prevent.<sup><a href="#fn4">[4]</a></sup>
      </p>

      <p>
        <strong>Cultural adoption.</strong> Success requires dedicated engineers building and maintaining
        harnesses. Not every engineer thrives in this mode. Engineers who love algorithmic puzzles and hands-on
        craft work struggle to go "agent-native." Product-focused developers who think in terms of outcomes
        rather than implementation adapt quickly. The personality fit for harness engineering is closer to
        technical program manager than to competitive programmer.<sup><a href="#fn1">[1]</a></sup>
      </p>

      <p>
        <strong>Standardization.</strong> Every team is building harnesses from scratch. There's no shared
        framework, no standard toolchain, no common vocabulary beyond <code>AGENTS.md</code>. The "service
        template" analogy from Bockeler's article suggests organizations will eventually adopt standardized
        harness templates as starting points, but this brings the same forking and synchronization challenges
        as any shared infrastructure.<sup><a href="#fn2">[2]</a></sup>
      </p>

      <h2><strong>Where this goes</strong></h2>

      <p>
        Bockeler's article notes that the harness designers she interviewed acknowledge their guardrails will
        "almost surely dissolve over time as models improve." This is probably right for the low-level
        constraints: as models get better at following conventions and avoiding obvious mistakes, the linter
        rules that catch those mistakes become less necessary.<sup><a href="#fn2">[2]</a></sup>
      </p>

      <p>
        But the higher-level components -- context architecture, verification systems, entropy management,
        feedback loops -- are not temporary scaffolding around weak models. They're permanent infrastructure
        for a new kind of software development. Better models will need less hand-holding but more
        sophisticated orchestration. The harness evolves; it doesn't disappear.
      </p>

      <p>
        A coding agent running LangChain's harness improved from 52.8% to 66.5% on a benchmark by modifying
        only the harness while keeping the underlying model constant. A 13.7-point gain from environment
        design alone.<sup><a href="#fn9">[9]</a></sup> That's the signal. The frontier isn't just better
        models. It's better harnesses around the same models. And unlike model training, harness engineering
        is something every team can do, starting now, with the models they already have.
      </p>

      <p>
        The engineer's job is no longer to write code. It's to design the environment in which code gets
        written correctly. That's not a demotion. It's the same shift that happened when we went from writing
        assembly to writing compilers, from managing servers to writing infrastructure-as-code. The abstraction
        layer moved up. The leverage moved up with it.
      </p>

      <div class="footnotes" id="footnotes">
        <h3><strong>Sources</strong></h3>
        <ol>
          <li id="fn1">
            Guo, "The Emerging Harness Engineering Playbook" -- Ignorance.ai (2026).
            <a href="https://www.ignorance.ai/p/the-emerging-harness-engineering"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn2">
            Bockeler, "Harness Engineering" -- Martin Fowler (2025).
            <a href="https://martinfowler.com/articles/exploring-gen-ai/harness-engineering.html"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn3">
            "Effective Context Engineering for AI Agents" -- Anthropic Engineering (2025).
            <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn4">
            "Codified Context: Infrastructure for AI Agents in a Complex Codebase" -- arXiv (2026).
            <a href="https://arxiv.org/abs/2602.20478"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn5">
            Young, "Effective Harnesses for Long-Running Agents" -- Anthropic Engineering (2025).
            <a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn6">
            Curlee, "The Inevitable Rise of Poor Code Quality in AI-Accelerated Codebases" -- SonarSource (2025).
            <a href="https://www.sonarsource.com/blog/the-inevitable-rise-of-poor-code-quality-in-ai-accelerated-codebases/"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn7">
            "How AI Generated Code Compounds Technical Debt" -- LeadDev (2025).
            <a href="https://leaddev.com/software-quality/how-ai-generated-code-accelerates-technical-debt"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn8">
            InfoQ -- "Anthropic Study: AI Coding Assistance Reduces Developer Skill Mastery by 17%" (February 2026).
            <a href="https://www.infoq.com/news/2026/02/ai-coding-skill-formation/"
              target="_blank" rel="noopener">Link</a>
          </li>
          <li id="fn9">
            "Improving Deep Agents with Harness Engineering" -- LangChain Blog (2026).
            <a href="https://blog.langchain.com/improving-deep-agents-with-harness-engineering/"
              target="_blank" rel="noopener">Link</a>
          </li>
        </ol>
      </div>

    </div>
  </div>

  <script src="/js/theme.js"></script>
</body>

</html>